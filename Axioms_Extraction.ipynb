{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNugMnhxwTIOSsa2qYnmCy2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LindaSekhoasha/Axiom-Extraction-From-Text-Using-Deep-Learning/blob/main/Axioms_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup (Installs • Imports • Config)"
      ],
      "metadata": {
        "id": "E6N4z-eWtGOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip -q install \"transformers>=4.40\" torch spacy datasets rdflib pandas --upgrade\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "HYUVD4Q2tQcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqANoKrzrZ4S"
      },
      "outputs": [],
      "source": [
        "# standard library\n",
        "import io\n",
        "import gzip\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional\n",
        "from urllib.parse import quote\n",
        "\n",
        "# third-party\n",
        "import pandas as pd\n",
        "import pydotplus\n",
        "import spacy\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "from google.colab import files\n",
        "from IPython.display import display, Image\n",
        "from rdflib import Graph, URIRef, Literal, Namespace, RDF, XSD\n",
        "from rdflib import Graph as RDFGraph\n",
        "from rdflib.namespace import OWL\n",
        "from rdflib.tools.rdf2dot import rdf2dot\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    LukeTokenizer,\n",
        "    LukeForEntityPairClassification,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "KtP_Le5ZtkHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_TAG = re.compile(r\"<[^>]+>\")\n",
        "def _clean_local(s: str) -> str:\n",
        "    s = _TAG.sub(\"\", s).strip().replace(\",\", \"\")\n",
        "    s = \"_\".join(s.split())\n",
        "    s = re.sub(r\"[^A-Za-z0-9_\\-./:]\", \"\", s)\n",
        "    return s\n",
        "\n",
        "def subject_uri_from_head(head: str):\n",
        "    h = _clean_local(head)\n",
        "    if not h:\n",
        "        return None\n",
        "    return _uri(DBR, h)\n",
        "\n",
        "def _uri(ns: Namespace, local: str) -> URIRef:\n",
        "    base = str(ns)\n",
        "    local = _clean_local(local)\n",
        "    return URIRef(base + quote(local, safe=\":/._-\"))\n",
        "\n",
        "def add_triplets_to_luke_graph(triplets, g: Graph, cls_name: str):\n",
        "    for h, r, t in triplets:\n",
        "        head = _clean_local(h)\n",
        "        pred = _clean_local(r)\n",
        "        tail = t.strip()\n",
        "        if not head or not pred:\n",
        "            continue\n",
        "        subj_uri = _uri(DBR, head)\n",
        "        pred_uri = _uri(EX, pred)\n",
        "        # simple literal/entity heuristic\n",
        "        if tail.isdigit() and len(tail) == 4:\n",
        "            obj_val = Literal(tail, datatype=XSD.gYear)\n",
        "        elif tail.isdigit():\n",
        "            obj_val = Literal(int(tail), datatype=XSD.integer)\n",
        "        else:\n",
        "            obj_val = _uri(DBR, tail)\n",
        "        g.add((subj_uri, pred_uri, obj_val))\n",
        "        g.add((subj_uri, RDF.type, DBO[cls_name]))\n",
        "\n",
        "def add_triplets_to_rebel_graph(triplets, cls_label):\n",
        "    \"\"\"\n",
        "    add extracted triplets to the RDF graph for a given dbpedia class.\n",
        "\n",
        "    args:\n",
        "        triplets (list of dict): [{'head': ..., 'type': ..., 'tail': ...}]\n",
        "        cls_label (int): Index of the dbpedia class from DBPEDIA_CLASSES\n",
        "    \"\"\"\n",
        "    cls_name = DBPEDIA_CLASSES[cls_label]\n",
        "    g = graphs_rebel[cls_name]\n",
        "\n",
        "    for t in triplets:\n",
        "        head = _clean_local(t['head'])\n",
        "        pred = _clean_local(t['type'])\n",
        "        tail = t['tail'].strip()\n",
        "\n",
        "        # skip if predicate vanished after cleaning\n",
        "        if not pred or not head:\n",
        "            continue\n",
        "\n",
        "        subj_uri = _uri(DBR, head)\n",
        "        pred_uri = _uri(EX, pred)\n",
        "\n",
        "        # tail: try to distinguish year, number, or entity; clean if entity\n",
        "        if tail.isdigit() and len(tail) == 4:\n",
        "            obj_val = Literal(tail, datatype=XSD.gYear)\n",
        "        elif tail.isdigit():\n",
        "            obj_val = Literal(int(tail), datatype=XSD.integer)\n",
        "        else:\n",
        "            obj_val = _uri(DBR, tail)\n",
        "\n",
        "        # add the triple\n",
        "        g.add((subj_uri, pred_uri, obj_val))\n",
        "\n",
        "        # assert rdf:type for the subject\n",
        "        g.add((subj_uri, RDF.type, DBO[cls_name]))\n",
        "\n",
        "# process the text from the rebel model and return the triplets\n",
        "def extract_triplets(text):\n",
        "    triplets = []\n",
        "    relation, subject, relation, object_ = '', '', '', ''\n",
        "    text = text.strip()\n",
        "    current = 'x'\n",
        "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
        "        if token == \"<triplet>\":\n",
        "            current = 't'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "                relation = ''\n",
        "            subject = ''\n",
        "        elif token == \"<subj>\":\n",
        "            current = 's'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "            object_ = ''\n",
        "        elif token == \"<obj>\":\n",
        "            current = 'o'\n",
        "            relation = ''\n",
        "        else:\n",
        "            if current == 't':\n",
        "                subject += ' ' + token\n",
        "            elif current == 's':\n",
        "                object_ += ' ' + token\n",
        "            elif current == 'o':\n",
        "                relation += ' ' + token\n",
        "    if subject != '' and relation != '' and object_ != '':\n",
        "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "    return triplets\n",
        "\n",
        "# prune candidate pairs\n",
        "def candidate_pairs_for_ents(ents):\n",
        "    \"\"\"\n",
        "    ents: List[(text, (s,e), label)]\n",
        "    returns: List[(h_text, (hs,he), t_text, (ts,te))]\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    for i, (ht, (hs,he), hl) in enumerate(ents):\n",
        "        for j, (tt, (ts,te), tl) in enumerate(ents):\n",
        "            if i == j:\n",
        "                continue\n",
        "            if ALLOWED_PAIRS and (hl, tl) not in ALLOWED_PAIRS:\n",
        "                continue\n",
        "            if MAX_CHAR_DISTANCE and abs(hs - ts) > MAX_CHAR_DISTANCE:\n",
        "                continue\n",
        "            pairs.append((ht, (hs,he), tt, (ts,te)))\n",
        "    if MAX_PAIRS_PER_DOC and len(pairs) > MAX_PAIRS_PER_DOC:\n",
        "        pairs = pairs[:MAX_PAIRS_PER_DOC]\n",
        "    return pairs\n",
        "\n",
        "# rdf graph visualization\n",
        "def visualize(graphs, doc_subjects, k: int = 5):\n",
        "    \"\"\"\n",
        "    build a subgraph containing ONLY triples whose subjects came from\n",
        "    dbpedia rows [0..k-1], and render it as a PNG.\n",
        "    \"\"\"\n",
        "    # collect target subject URIs for the first k rows\n",
        "    targets = set()\n",
        "    for gidx in range(min(k, len(texts))):\n",
        "        targets |= doc_subjects.get(gidx, set())\n",
        "\n",
        "    # build subgraph: include any triple whose subject is in targets\n",
        "    sub = RDFGraph()\n",
        "    sub.bind(\"ex\", EX); sub.bind(\"dbr\", DBR); sub.bind(\"dbo\", DBO)\n",
        "\n",
        "    for g in graphs.values():\n",
        "        for s, p, o in g:\n",
        "            if s in targets:\n",
        "                sub.add((s, p, o))\n",
        "\n",
        "    if len(sub) == 0:\n",
        "        print(f\"No triples found for the first {k} rows.\")\n",
        "        return\n",
        "\n",
        "    stream = io.StringIO()\n",
        "    rdf2dot(sub, stream)\n",
        "    dg = pydotplus.graph_from_dot_data(stream.getvalue())\n",
        "    display(Image(dg.create_png()))\n",
        "\n",
        "def _is_uri(x): return isinstance(x, URIRef)\n",
        "def _is_lit(x): return isinstance(x, Literal)\n",
        "\n",
        "def compute_metrics_for_graph(g, cls_name, DBO):\n",
        "    cls_uri = URIRef(str(DBO) + cls_name)\n",
        "\n",
        "    # individuals: subjects that are typed as this class\n",
        "    individuals = {s for s, p, o in g.triples((None, RDF.type, cls_uri))}\n",
        "\n",
        "    # basic counts\n",
        "    uniques_s = set()\n",
        "    uniques_p = set()\n",
        "    uniques_o = set()\n",
        "    obj_triples = 0\n",
        "    data_triples = 0\n",
        "\n",
        "    for s, p, o in g:\n",
        "        uniques_s.add(s); uniques_p.add(p); uniques_o.add(o)\n",
        "        if _is_lit(o):\n",
        "            data_triples += 1\n",
        "        else:\n",
        "            obj_triples += 1\n",
        "\n",
        "    total_props = obj_triples + data_triples\n",
        "    relationship_richness = (obj_triples / total_props) if total_props else 0.0\n",
        "    attribute_richness   = (data_triples / max(len(individuals), 1)) if len(individuals) else 0.0\n",
        "\n",
        "    # degrees over individuals (within this graph)\n",
        "    out_deg = defaultdict(int)\n",
        "    in_deg  = defaultdict(int)\n",
        "    for s, p, o in g:\n",
        "        if s in individuals:\n",
        "            out_deg[s] += 1\n",
        "        if o in individuals:\n",
        "            in_deg[o] += 1\n",
        "    if individuals:\n",
        "        avg_degree = sum(out_deg[i] + in_deg[i] for i in individuals) / len(individuals)\n",
        "    else:\n",
        "        avg_degree = 0.0\n",
        "\n",
        "    return {\n",
        "        \"class\": cls_name,\n",
        "        \"triples\": len(g),\n",
        "        \"individuals\": len(individuals),\n",
        "        \"unique_subjects\": len(uniques_s),\n",
        "        \"unique_predicates\": len(uniques_p),\n",
        "        \"unique_objects\": len(uniques_o),\n",
        "        \"object_triples\": obj_triples,\n",
        "        \"data_triples\": data_triples,\n",
        "        \"relationship_richness\": relationship_richness,\n",
        "        \"attribute_richness\": attribute_richness,\n",
        "        \"avg_degree_individuals\": avg_degree,\n",
        "    }"
      ],
      "metadata": {
        "id": "eDGbq9hUtlxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONFIG & Variables"
      ],
      "metadata": {
        "id": "3jcJIk17uKSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LUKE_MODEL          = \"studio-ousia/luke-large-finetuned-tacred\"  # pretrained, non-LLM baseline\n",
        "REBEL_MODEL         = \"Babelscape/rebel-large\"                    # pretrained, LLMs\n",
        "\n",
        "CONF_THRESHOLD      = 0.50                    # accept a relation only if softmax prob >= this\n",
        "MAX_DOCS            = 120_000                      # train[:N] (used subset of dbpedia_14 do to hardware limitations)\n",
        "BATCH_SIZE          = 128                     # number of texts per loop (NER + inference per pair)\n",
        "NUM_RETURNS         = 3\n",
        "PRINT_EVERY         = 100\n",
        "OUTPUT_DIR_LUKE     = \"out_luke\"\n",
        "OUTPUT_DIR_REBEL    = \"out_rebel\"\n",
        "SAVE_XML_CLASS      = \"Company\"               # set None to skip\n",
        "VALID_NER_LABELS    = {\"PERSON\",\"ORG\",\"GPE\",\"PRODUCT\",\"FAC\",\"WORK_OF_ART\"}\n",
        "MAX_PAIRS_PER_DOC   = 30                      # cap pairs per doc for speed; set None for all\n",
        "doc_subjects_luke   = {}  # global_doc_idx -> set of subject URIs added from that row\n",
        "doc_subjects_rebel  = {}  # global_doc_idx -> set of subject URIs added from that row\n",
        "\n",
        "ALLOWED_PAIRS = {\n",
        "    (\"PERSON\",\"ORG\"), (\"ORG\",\"PERSON\"),\n",
        "    (\"PERSON\",\"GPE\"), (\"GPE\",\"PERSON\"),\n",
        "    (\"ORG\",\"ORG\"), (\"ORG\",\"GPE\"), (\"GPE\",\"ORG\"),\n",
        "}\n",
        "MAX_CHAR_DISTANCE = 120     # skip far-apart mentions; set None to disable\n",
        "PAIR_BATCH        = 128     # pairs per LUKE forward; tune 32–128 on A100"
      ],
      "metadata": {
        "id": "YGSsYsuquLa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET: [dbpedia_14](https://huggingface.co/datasets/fancyzhx/dbpedia_14/viewer/dbpedia_14/train?views%5B%5D=train&row=5)\n",
        "A supervised text-classification benchmark built from DBpedia article abstracts. Each sample is short text (title + abstract) with one of 14 ontology classes (e.g., Company, Person, Place). The Hugging Face variant fancyzhx/dbpedia_14 mirrors the widely used setup from the literature: ~560k training docs and 70k test docs with label IDs and class names."
      ],
      "metadata": {
        "id": "_VR7jpKo7pIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"fancyzhx/dbpedia_14\", split=f\"train[:{MAX_DOCS}]\")\n",
        "texts = [ex[\"content\"] for ex in dataset]\n",
        "labels = [ex[\"label\"] for ex in dataset]\n",
        "\n",
        "DBPEDIA_CLASSES = dataset.features[\"label\"].names\n",
        "print(f\"DBPEDIA_CLASSES (ONTOLOGIES):\\n{DBPEDIA_CLASSES}\")"
      ],
      "metadata": {
        "id": "YYs6o15W7p94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NAMESPACES\n",
        "EX  = Namespace(\"http://example.org/ontology/\")\n",
        "DBR = Namespace(\"http://dbpedia.org/resource/\")\n",
        "DBO = Namespace(\"http://dbpedia.org/ontology/\")\n",
        "\n",
        "graphs_luke = {cls: Graph() for cls in DBPEDIA_CLASSES}\n",
        "graphs_rebel = {cls: Graph() for cls in DBPEDIA_CLASSES}"
      ],
      "metadata": {
        "id": "2Cso6g2C8aqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batched spaCy NER once for all texts - LUKE\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"attribute_ruler\"])\n",
        "nlp.max_length = max(len(t) for t in texts) + 10\n",
        "\n",
        "ents_by_doc = []  # List[List[ (ent_text, (start_char,end_char), ent_label) ]]\n",
        "for doc in nlp.pipe(texts, batch_size=256, n_process=2):\n",
        "    ents = [(e.text, (e.start_char, e.end_char), e.label_) for e in doc.ents if e.label_ in VALID_NER_LABELS]\n",
        "    ents_by_doc.append(ents)"
      ],
      "metadata": {
        "id": "LzYwwQh28c_i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}