{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMi0NDTD+7ARy5EStbSGqVX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LindaSekhoasha/Axiom-Extraction-From-Text-Using-Deep-Learning/blob/main/Axioms_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup (Installs • Imports • Config)"
      ],
      "metadata": {
        "id": "E6N4z-eWtGOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip -q install \"transformers>=4.40\" torch spacy datasets rdflib pandas --upgrade\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "HYUVD4Q2tQcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqANoKrzrZ4S"
      },
      "outputs": [],
      "source": [
        "# standard library\n",
        "import io\n",
        "import gzip\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional\n",
        "from urllib.parse import quote\n",
        "\n",
        "# third-party\n",
        "import pandas as pd\n",
        "import pydotplus\n",
        "import spacy\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "from google.colab import files\n",
        "from IPython.display import display, Image\n",
        "from rdflib import Graph, URIRef, Literal, Namespace, RDF, XSD\n",
        "from rdflib import Graph as RDFGraph\n",
        "from rdflib.namespace import OWL\n",
        "from rdflib.tools.rdf2dot import rdf2dot\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    LukeTokenizer,\n",
        "    LukeForEntityPairClassification,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "KtP_Le5ZtkHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_TAG = re.compile(r\"<[^>]+>\")\n",
        "def _clean_local(s: str) -> str:\n",
        "    s = _TAG.sub(\"\", s).strip().replace(\",\", \"\")\n",
        "    s = \"_\".join(s.split())\n",
        "    s = re.sub(r\"[^A-Za-z0-9_\\-./:]\", \"\", s)\n",
        "    return s\n",
        "\n",
        "def subject_uri_from_head(head: str):\n",
        "    h = _clean_local(head)\n",
        "    if not h:\n",
        "        return None\n",
        "    return _uri(DBR, h)\n",
        "\n",
        "def _uri(ns: Namespace, local: str) -> URIRef:\n",
        "    base = str(ns)\n",
        "    local = _clean_local(local)\n",
        "    return URIRef(base + quote(local, safe=\":/._-\"))\n",
        "\n",
        "def add_triplets_to_luke_graph(triplets, g: Graph, cls_name: str):\n",
        "    for h, r, t in triplets:\n",
        "        head = _clean_local(h)\n",
        "        pred = _clean_local(r)\n",
        "        tail = t.strip()\n",
        "        if not head or not pred:\n",
        "            continue\n",
        "        subj_uri = _uri(DBR, head)\n",
        "        pred_uri = _uri(EX, pred)\n",
        "        # simple literal/entity heuristic\n",
        "        if tail.isdigit() and len(tail) == 4:\n",
        "            obj_val = Literal(tail, datatype=XSD.gYear)\n",
        "        elif tail.isdigit():\n",
        "            obj_val = Literal(int(tail), datatype=XSD.integer)\n",
        "        else:\n",
        "            obj_val = _uri(DBR, tail)\n",
        "        g.add((subj_uri, pred_uri, obj_val))\n",
        "        g.add((subj_uri, RDF.type, DBO[cls_name]))\n",
        "\n",
        "def add_triplets_to_rebel_graph(triplets, cls_label):\n",
        "    \"\"\"\n",
        "    add extracted triplets to the RDF graph for a given dbpedia class.\n",
        "\n",
        "    args:\n",
        "        triplets (list of dict): [{'head': ..., 'type': ..., 'tail': ...}]\n",
        "        cls_label (int): Index of the dbpedia class from DBPEDIA_CLASSES\n",
        "    \"\"\"\n",
        "    cls_name = DBPEDIA_CLASSES[cls_label]\n",
        "    g = graphs_rebel[cls_name]\n",
        "\n",
        "    for t in triplets:\n",
        "        head = _clean_local(t['head'])\n",
        "        pred = _clean_local(t['type'])\n",
        "        tail = t['tail'].strip()\n",
        "\n",
        "        # skip if predicate vanished after cleaning\n",
        "        if not pred or not head:\n",
        "            continue\n",
        "\n",
        "        subj_uri = _uri(DBR, head)\n",
        "        pred_uri = _uri(EX, pred)\n",
        "\n",
        "        # tail: try to distinguish year, number, or entity; clean if entity\n",
        "        if tail.isdigit() and len(tail) == 4:\n",
        "            obj_val = Literal(tail, datatype=XSD.gYear)\n",
        "        elif tail.isdigit():\n",
        "            obj_val = Literal(int(tail), datatype=XSD.integer)\n",
        "        else:\n",
        "            obj_val = _uri(DBR, tail)\n",
        "\n",
        "        # add the triple\n",
        "        g.add((subj_uri, pred_uri, obj_val))\n",
        "\n",
        "        # assert rdf:type for the subject\n",
        "        g.add((subj_uri, RDF.type, DBO[cls_name]))\n",
        "\n",
        "# process the text from the rebel model and return the triplets\n",
        "def extract_triplets(text):\n",
        "    triplets = []\n",
        "    relation, subject, relation, object_ = '', '', '', ''\n",
        "    text = text.strip()\n",
        "    current = 'x'\n",
        "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
        "        if token == \"<triplet>\":\n",
        "            current = 't'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "                relation = ''\n",
        "            subject = ''\n",
        "        elif token == \"<subj>\":\n",
        "            current = 's'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "            object_ = ''\n",
        "        elif token == \"<obj>\":\n",
        "            current = 'o'\n",
        "            relation = ''\n",
        "        else:\n",
        "            if current == 't':\n",
        "                subject += ' ' + token\n",
        "            elif current == 's':\n",
        "                object_ += ' ' + token\n",
        "            elif current == 'o':\n",
        "                relation += ' ' + token\n",
        "    if subject != '' and relation != '' and object_ != '':\n",
        "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "    return triplets\n",
        "\n",
        "# prune candidate pairs\n",
        "def candidate_pairs_for_ents(ents):\n",
        "    \"\"\"\n",
        "    ents: List[(text, (s,e), label)]\n",
        "    returns: List[(h_text, (hs,he), t_text, (ts,te))]\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    for i, (ht, (hs,he), hl) in enumerate(ents):\n",
        "        for j, (tt, (ts,te), tl) in enumerate(ents):\n",
        "            if i == j:\n",
        "                continue\n",
        "            if ALLOWED_PAIRS and (hl, tl) not in ALLOWED_PAIRS:\n",
        "                continue\n",
        "            if MAX_CHAR_DISTANCE and abs(hs - ts) > MAX_CHAR_DISTANCE:\n",
        "                continue\n",
        "            pairs.append((ht, (hs,he), tt, (ts,te)))\n",
        "    if MAX_PAIRS_PER_DOC and len(pairs) > MAX_PAIRS_PER_DOC:\n",
        "        pairs = pairs[:MAX_PAIRS_PER_DOC]\n",
        "    return pairs\n",
        "\n",
        "# rdf graph visualization\n",
        "def visualize(graphs, doc_subjects, k: int = 5):\n",
        "    \"\"\"\n",
        "    build a subgraph containing ONLY triples whose subjects came from\n",
        "    dbpedia rows [0..k-1], and render it as a PNG.\n",
        "    \"\"\"\n",
        "    # collect target subject URIs for the first k rows\n",
        "    targets = set()\n",
        "    for gidx in range(min(k, len(texts))):\n",
        "        targets |= doc_subjects.get(gidx, set())\n",
        "\n",
        "    # build subgraph: include any triple whose subject is in targets\n",
        "    sub = RDFGraph()\n",
        "    sub.bind(\"ex\", EX); sub.bind(\"dbr\", DBR); sub.bind(\"dbo\", DBO)\n",
        "\n",
        "    for g in graphs.values():\n",
        "        for s, p, o in g:\n",
        "            if s in targets:\n",
        "                sub.add((s, p, o))\n",
        "\n",
        "    if len(sub) == 0:\n",
        "        print(f\"No triples found for the first {k} rows.\")\n",
        "        return\n",
        "\n",
        "    stream = io.StringIO()\n",
        "    rdf2dot(sub, stream)\n",
        "    dg = pydotplus.graph_from_dot_data(stream.getvalue())\n",
        "    display(Image(dg.create_png()))\n",
        "\n",
        "def _is_uri(x): return isinstance(x, URIRef)\n",
        "def _is_lit(x): return isinstance(x, Literal)\n",
        "\n",
        "def compute_metrics_for_graph(g, cls_name, DBO):\n",
        "    cls_uri = URIRef(str(DBO) + cls_name)\n",
        "\n",
        "    # individuals: subjects that are typed as this class\n",
        "    individuals = {s for s, p, o in g.triples((None, RDF.type, cls_uri))}\n",
        "\n",
        "    # basic counts\n",
        "    uniques_s = set()\n",
        "    uniques_p = set()\n",
        "    uniques_o = set()\n",
        "    obj_triples = 0\n",
        "    data_triples = 0\n",
        "\n",
        "    for s, p, o in g:\n",
        "        uniques_s.add(s); uniques_p.add(p); uniques_o.add(o)\n",
        "        if _is_lit(o):\n",
        "            data_triples += 1\n",
        "        else:\n",
        "            obj_triples += 1\n",
        "\n",
        "    total_props = obj_triples + data_triples\n",
        "    relationship_richness = (obj_triples / total_props) if total_props else 0.0\n",
        "    attribute_richness   = (data_triples / max(len(individuals), 1)) if len(individuals) else 0.0\n",
        "\n",
        "    # degrees over individuals (within this graph)\n",
        "    out_deg = defaultdict(int)\n",
        "    in_deg  = defaultdict(int)\n",
        "    for s, p, o in g:\n",
        "        if s in individuals:\n",
        "            out_deg[s] += 1\n",
        "        if o in individuals:\n",
        "            in_deg[o] += 1\n",
        "    if individuals:\n",
        "        avg_degree = sum(out_deg[i] + in_deg[i] for i in individuals) / len(individuals)\n",
        "    else:\n",
        "        avg_degree = 0.0\n",
        "\n",
        "    return {\n",
        "        \"class\": cls_name,\n",
        "        \"triples\": len(g),\n",
        "        \"individuals\": len(individuals),\n",
        "        \"unique_subjects\": len(uniques_s),\n",
        "        \"unique_predicates\": len(uniques_p),\n",
        "        \"unique_objects\": len(uniques_o),\n",
        "        \"object_triples\": obj_triples,\n",
        "        \"data_triples\": data_triples,\n",
        "        \"relationship_richness\": relationship_richness,\n",
        "        \"attribute_richness\": attribute_richness,\n",
        "        \"avg_degree_individuals\": avg_degree,\n",
        "    }"
      ],
      "metadata": {
        "id": "eDGbq9hUtlxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONFIG & Variables"
      ],
      "metadata": {
        "id": "3jcJIk17uKSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LUKE_MODEL          = \"studio-ousia/luke-large-finetuned-tacred\"  # pretrained, non-LLM baseline\n",
        "REBEL_MODEL         = \"Babelscape/rebel-large\"                    # pretrained, LLMs\n",
        "\n",
        "CONF_THRESHOLD      = 0.50                    # accept a relation only if softmax prob >= this\n",
        "MAX_DOCS            = 120_000                      # train[:N] (used subset of dbpedia_14 do to hardware limitations)\n",
        "BATCH_SIZE          = 128                     # number of texts per loop (NER + inference per pair)\n",
        "NUM_RETURNS         = 3\n",
        "PRINT_EVERY         = 100\n",
        "OUTPUT_DIR_LUKE     = \"out_luke\"\n",
        "OUTPUT_DIR_REBEL    = \"out_rebel\"\n",
        "SAVE_XML_CLASS      = \"Company\"               # set None to skip\n",
        "VALID_NER_LABELS    = {\"PERSON\",\"ORG\",\"GPE\",\"PRODUCT\",\"FAC\",\"WORK_OF_ART\"}\n",
        "MAX_PAIRS_PER_DOC   = 30                      # cap pairs per doc for speed; set None for all\n",
        "doc_subjects_luke   = {}  # global_doc_idx -> set of subject URIs added from that row\n",
        "doc_subjects_rebel  = {}  # global_doc_idx -> set of subject URIs added from that row\n",
        "\n",
        "ALLOWED_PAIRS = {\n",
        "    (\"PERSON\",\"ORG\"), (\"ORG\",\"PERSON\"),\n",
        "    (\"PERSON\",\"GPE\"), (\"GPE\",\"PERSON\"),\n",
        "    (\"ORG\",\"ORG\"), (\"ORG\",\"GPE\"), (\"GPE\",\"ORG\"),\n",
        "}\n",
        "MAX_CHAR_DISTANCE = 120     # skip far-apart mentions; set None to disable\n",
        "PAIR_BATCH        = 128     # pairs per LUKE forward; tune 32–128 on A100"
      ],
      "metadata": {
        "id": "YGSsYsuquLa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET: [dbpedia_14](https://huggingface.co/datasets/fancyzhx/dbpedia_14/viewer/dbpedia_14/train?views%5B%5D=train&row=5)\n",
        "---\n",
        "A supervised text-classification benchmark built from DBpedia article abstracts. Each sample is short text (title + abstract) with one of 14 ontology classes (e.g., Company, Person, Place). The Hugging Face variant `fancyzhx/dbpedia_14` mirrors the widely used setup from the literature: ~560k training docs and 70k test docs with label IDs and class names."
      ],
      "metadata": {
        "id": "_VR7jpKo7pIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"fancyzhx/dbpedia_14\", split=f\"train[:{MAX_DOCS}]\")\n",
        "texts = [ex[\"content\"] for ex in dataset]\n",
        "labels = [ex[\"label\"] for ex in dataset]\n",
        "\n",
        "DBPEDIA_CLASSES = dataset.features[\"label\"].names\n",
        "print(f\"DBPEDIA_CLASSES (ONTOLOGIES):\\n{DBPEDIA_CLASSES}\")"
      ],
      "metadata": {
        "id": "YYs6o15W7p94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NAMESPACES\n",
        "EX  = Namespace(\"http://example.org/ontology/\")\n",
        "DBR = Namespace(\"http://dbpedia.org/resource/\")\n",
        "DBO = Namespace(\"http://dbpedia.org/ontology/\")\n",
        "\n",
        "graphs_luke = {cls: Graph() for cls in DBPEDIA_CLASSES}\n",
        "graphs_rebel = {cls: Graph() for cls in DBPEDIA_CLASSES}"
      ],
      "metadata": {
        "id": "2Cso6g2C8aqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batched spaCy NER once for all texts - LUKE\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\"parser\",\"lemmatizer\",\"attribute_ruler\"])\n",
        "nlp.max_length = max(len(t) for t in texts) + 10\n",
        "\n",
        "ents_by_doc = []  # List[List[ (ent_text, (start_char,end_char), ent_label) ]]\n",
        "for doc in nlp.pipe(texts, batch_size=256, n_process=2):\n",
        "    ents = [(e.text, (e.start_char, e.end_char), e.label_) for e in doc.ents if e.label_ in VALID_NER_LABELS]\n",
        "    ents_by_doc.append(ents)"
      ],
      "metadata": {
        "id": "LzYwwQh28c_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LUKE (TACRED)\n",
        "---\n",
        "\n",
        "Implements an entity-aware relation extraction workflow using `LukeTokenizer` and `LukeForEntityPairClassification` from Hugging Face Transformers (Yamada et al., 2020 [[ACL Anthology]](https://aclanthology.org/2020.emnlp-main.523/)) and the TACRED schema (Zhang et al., 2017 [[ACL Anthology]](https://aclanthology.org/D17-1004/)).\n",
        "It encodes character-span entity pairs directly (no offset remapping), tokenizes with `task=\"entity_pair_classification\"`, and batches through LUKE on GPU with AMP acceleration.\n",
        "Logits are softmax-normalized, mapped to `id2label`, and filtered by `conf_threshold` to discard low-confidence or `no_relation` predictions.\n",
        "Each document’s pre-detected NER entities (restricted to `valid_ner_labels`) are enumerated into head–tail **candidate pairs**, truncated by `MAX_PAIRS_PER_DOC`, then processed in mini-batches (`PAIR_BATCH`).\n",
        "The resulting `(head, relation, tail)` triplets are aligned to source documents and inserted into per-class DBpedia graphs for ontology construction — following LUKE’s entity-aware self-attention design (Yamada et al., 2020) and TACRED’s labeled-relation benchmark protocol (Zhang et al., 2017).\n"
      ],
      "metadata": {
        "id": "6YtD70R2Wjtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Triplet:\n",
        "    head: str\n",
        "    rel: str\n",
        "    tail: str\n",
        "\n",
        "class LukeExtractor:\n",
        "    \"\"\"\n",
        "    class wrapper for LUKE TACRED relation classifier.\n",
        "    uses character-based entity spans directly (no offset mapping).\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name=LUKE_MODEL, conf_threshold=CONF_THRESHOLD,\n",
        "                 valid_ner_labels=VALID_NER_LABELS, device: Optional[str] = \"cuda\"):\n",
        "        self.device = \"cuda\" if (device == \"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
        "        self.tok = LukeTokenizer.from_pretrained(model_name, task=\"entity_pair_classification\")\n",
        "        self.model = LukeForEntityPairClassification.from_pretrained(model_name).to(self.device).eval()\n",
        "        self.conf_threshold = conf_threshold\n",
        "        self.valid_ner = set(valid_ner_labels)\n",
        "\n",
        "    # vectorized forward for a batch of pairs\n",
        "    def _classify_pairs_batch(self, texts_list, spans_list):\n",
        "        \"\"\"\n",
        "        texts_list: List[str]\n",
        "        spans_list: List[[ (hs,he), (ts,te) ]]\n",
        "        returns: (pred_ids, confs)\n",
        "        \"\"\"\n",
        "        inputs = self.tok(\n",
        "            texts_list,\n",
        "            entity_spans=spans_list,\n",
        "            add_prefix_space=True,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=256,\n",
        "        )\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=(self.device==\"cuda\")):\n",
        "            logits = self.model(**inputs).logits\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        confs, pred_ids = probs.max(dim=-1)\n",
        "        return pred_ids.cpu().tolist(), confs.cpu().tolist()\n",
        "\n",
        "    def _pairs(self, text: str) -> List[Tuple[str, Tuple[int,int], str, Tuple[int,int]]]:\n",
        "        # produce candidate entity pairs + their **character** spans\n",
        "        doc = self.ner(text)\n",
        "        ents = [e for e in doc.ents if e.label_ in self.valid_ner]\n",
        "        pairs = []\n",
        "        for i, h in enumerate(ents):\n",
        "            for j, t in enumerate(ents):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                pairs.append((h.text, (h.start_char, h.end_char), t.text, (t.start_char, t.end_char)))\n",
        "        if MAX_PAIRS_PER_DOC and len(pairs) > MAX_PAIRS_PER_DOC:\n",
        "            pairs = pairs[:MAX_PAIRS_PER_DOC]\n",
        "        return pairs\n",
        "\n",
        "    def extract_batch(self, texts: List[str], ents_slice: List[list]) -> List[List[Triplet]]:\n",
        "        \"\"\"\n",
        "        ents_slice: precomputed entities for these texts (subset of ents_by_doc)\n",
        "        returns: List[List[Triplet]] aligned to `texts`\n",
        "        \"\"\"\n",
        "        results = [[] for _ in range(len(texts))]\n",
        "\n",
        "        # flatten pairs across docs\n",
        "        flat_texts, flat_spans, index_map, head_cache, tail_cache = [], [], [], [], []\n",
        "        for di, ents in enumerate(ents_slice):\n",
        "            pairs = candidate_pairs_for_ents(ents)\n",
        "            for (h_txt,(h_s,h_e), t_txt,(t_s,t_e)) in pairs:\n",
        "                flat_texts.append(texts[di])\n",
        "                flat_spans.append([(h_s,h_e), (t_s,t_e)])\n",
        "                index_map.append(di)\n",
        "                head_cache.append(h_txt)\n",
        "                tail_cache.append(t_txt)\n",
        "\n",
        "        # run LUKE in mini-batches\n",
        "        id2label = self.model.config.id2label\n",
        "        for k in range(0, len(flat_texts), PAIR_BATCH):\n",
        "            b_texts = flat_texts[k:k+PAIR_BATCH]\n",
        "            b_spans = flat_spans[k:k+PAIR_BATCH]\n",
        "            pred_ids, confs = self._classify_pairs_batch(b_texts, b_spans)\n",
        "            for off, (pid, conf) in enumerate(zip(pred_ids, confs)):\n",
        "                lbl = id2label[pid]\n",
        "                if lbl != \"no_relation\" and conf >= self.conf_threshold:\n",
        "                    di = index_map[k + off]\n",
        "                    h_txt = head_cache[k + off]\n",
        "                    t_txt = tail_cache[k + off]\n",
        "                    results[di].append(Triplet(h_txt, lbl, t_txt))\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "tz15GY5HZb2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Luke model loop"
      ],
      "metadata": {
        "id": "q7__LrwNptON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(OUTPUT_DIR_LUKE, exist_ok=True)\n",
        "# Prepare graphs per class\n",
        "for g in graphs_luke.values():\n",
        "    g.bind(\"ex\", EX); g.bind(\"dbr\", DBR); g.bind(\"dbo\", DBO)\n",
        "\n",
        "extractor = LukeExtractor()\n",
        "\n",
        "# Process texts in batches\n",
        "total = len(texts)\n",
        "for i in range(0, total, BATCH_SIZE):\n",
        "    batch_texts  = texts[i:i+BATCH_SIZE]\n",
        "    batch_labels = labels[i:i+BATCH_SIZE]\n",
        "\n",
        "    ents_slice = ents_by_doc[i:i+BATCH_SIZE]\n",
        "    per_doc_trips = extractor.extract_batch(batch_texts, ents_slice)\n",
        "\n",
        "    start = i  # batch start index in the global dataset\n",
        "\n",
        "    for j, (trips, lbl) in enumerate(zip(per_doc_trips, batch_labels)):\n",
        "        if not trips:\n",
        "            continue\n",
        "        cls_name = DBPEDIA_CLASSES[lbl]\n",
        "\n",
        "        # record subjects for this **global** row index\n",
        "        gidx = start + j\n",
        "\n",
        "        subs = {subject_uri_from_head(t.head) for t in trips if _clean_local(t.head)}\n",
        "        if subs:\n",
        "            doc_subjects_luke[gidx] = subs\n",
        "\n",
        "        # add to the class graph (unchanged)\n",
        "        add_triplets_to_luke_graph([(t.head, t.rel, t.tail) for t in trips], graphs_luke[cls_name], cls_name)\n",
        "\n",
        "\n",
        "    done = min(i + len(batch_texts), total)\n",
        "    if PRINT_EVERY and done % PRINT_EVERY == 0:\n",
        "        print(f\"[INFO] Processed {done}/{total} docs\")"
      ],
      "metadata": {
        "id": "DVzFMiTnYGCJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}